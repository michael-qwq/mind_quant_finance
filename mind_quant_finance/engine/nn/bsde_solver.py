from typing import Optional, Callable
from abc import ABC, abstractmethod

import mindspore as ms
import mindspore.numpy as np
from mindspore import nn, ops
from mindspore import dtype as mstype

from mind_quant_finance.math.random_ops.random import generate_mc_normal_draws


# This block is used to solve the gradient of g_fn
class BSDEGradientWrapper(nn.Cell):
    def __init__(self, param, g_fn: Callable):
        r"""
            A Wrapper for SDE Backend Limitation to Calculate the Pilot Gradients.

            Args:
                - **param** (dict) - Some parameters organized bt the dictionary, which help the calculation of g_fn.
                - **g_fn** (Callable) - The backend limitation function.

            Inputs:
                - **x** (Tensor) - The Xt, with shape=(batch_size, dim).

            Outputs:
                - **gx** (Tensor) - The value of g(x), which is the backend limitation of the BSDE problem.

            Reference: [1] Raissi, M. . (2018). Forward-Backward Stochastic Neural Networks: Deep Learning of
                           High-dimensional Partial Differential Equations.  10.48550/arXiv.1804.07010.
        """
        super(BSDEGradientWrapper, self).__init__()
        self._param = param
        self._g_fn = g_fn

    def construct(self, x):
        return self._g_fn(x, self._param)


class BSDEApproximateNet(nn.Cell):
    def __init__(self, dim, hidden_dim, net_depth, net_activation, net_initializer='Normal'):
        r"""
            Calculate the Approximation of the Unknown Solution Yt = u(t, Xt).

            Args:
                - **dim** (int) - The dimension of Xt.
                - **hidden_dim** (int) - The dimension of the hidden layer of the MLP.
                - **net_depth** (int) - The depth of the MLP.
                - **net_activation** (ms.nn.Cell) - The activate function of the MLP.
                - **net_initializer** (str) - The initializer of the MLP. Default: 'Normal'.

            Inputs:
                - **r** (Tensor) - The Xt, with shape=(batch_size, dim).
                - **t** (Tensor) - The timestamp t, with shape=(batch_size, 1).

            Outputs:
                - **sol** (Tensor) - The approximate value of u(t, Xt).

            Reference: [1] Raissi, M. . (2018). Forward-Backward Stochastic Neural Networks: Deep Learning of
                           High-dimensional Partial Differential Equations.  10.48550/arXiv.1804.07010.
        """
        super(BSDEApproximateNet, self).__init__()
        self.projection = nn.Dense(dim + 1, hidden_dim, weight_init=net_initializer,
                                   activation=net_activation, has_bias=True)
        self.hidden_layers = nn.SequentialCell()
        for i in range(net_depth):
            self.hidden_layers.append(
                nn.Dense(hidden_dim, hidden_dim, weight_init=net_initializer,
                         has_bias=True, activation=net_activation)
            )
        self.regression = nn.Dense(hidden_dim, 1)
        self.ops_concat = ops.Concat(axis=1)

    def construct(self, r, t):
        inputs = self.ops_concat([r, t])
        proj = self.projection(inputs)
        embedding = self.hidden_layers(proj)
        sol = self.regression(embedding)
        return sol


class BSDEGeneratePath(nn.Cell):
    def __init__(self, batch_size: int,
                 terminal_time: float,
                 n_time: int,
                 dim: int,
                 r0: ms.Tensor,
                 param: dict,
                 weight_g: float,
                 mu_fn: Callable,
                 sigma_fn: Callable,
                 phi_fn: Callable,
                 g_fn: Callable,
                 net_depth: Optional[int] = 4,
                 net_hidden_dim: Optional[int] = 256,
                 net_activation: Optional[nn.Cell] = nn.Tanh(),
                 net_initializer: Optional[str] = 'Normal',
                 dtype=mstype.float32
                 ):
        r"""
            Generate Path of the SDE Process with Euler-Maruyama Scheme.

            Args:
                - **batch_size** (int) - The batch_size of the solver.
                - **terminal_time** (float) - The terminal time point of the sde process.
                - **n_time** (int) - The amount of timestamp of the generated path. Please note that t=0 is not included
                                     in this input statistics, which means that the path generated by the solver will
                                     have n_time+1 timestamps with t=0 add to the beginning of path automatically.
                - **dim** (int) - The dimension of the variable Xt.
                - **r0** (Tensor) - The initial state, namely X0.
                - **param** (dict) - Some parameters organized bt the dictionary, which help the calculation of
                                     mu_fn, sigma_fn, phi_fn, g_fn.
                - **weight_g** (float) - Hyper-parameter for the loss function, the weight of backend limitation in the
                                         loss function.
                - **mu_fn** (Callable) - The time-relative increment function of Xt.
                - **sigma_fn** (Callable) - The brownian-relative increment function.
                - **phi_fn** (Callable) - The time-relative increment function of Yt.
                - **g_fn** (Callable) - The backend limitation function.
                - **net_depth** (int) - The depth of the u(Xt,t)-approximation MLP. Default: 4.
                - **net_hidden_dim** (int) - The dimension of hidden layer of the u(Xt,t)-approximation MLP.
                                             Default: 256.
                - **net_activation** (nn.Cell) - The activation function of the u(Xt,t)-approximation MLP.
                                                 Default: nn.Tanh().
                - **net_initializer** (str) - The initializer of the MLP. Default: 'Normal'.
                - **dtype** (mstype) - The data type of the solver. Default: mstype.float32.

            Inputs:
                - **et** (Tensor) - The timestamp t, with shape=(batch_size, n_time+1, 1).
                - **ew** (Tensor) - The accumulated brownian motion value, with shape=(batch_size, n_time+1, 1).

            Outputs:
                - **sol** (Tensor) - The value of loss function of the processed generation.

            Reference: [1] Raissi, M. . (2018). Forward-Backward Stochastic Neural Networks: Deep Learning of
                           High-dimensional Partial Differential Equations.  10.48550/arXiv.1804.07010.
        """
        super(BSDEGeneratePath, self).__init__()
        self._dim = dim
        self._n_time = n_time
        self._terminal_time = terminal_time
        self._batch_size = batch_size
        self._param = param
        self._mu = mu_fn
        self._sigma = sigma_fn
        self._phi = phi_fn
        self._g_net = BSDEGradientWrapper(self._param, g_fn)
        self._dtype = dtype
        self._r0 = r0
        self._weight_g = weight_g
        # r0.shape = (batch_size, dim)
        self.grad_op = ops.GradOperation()
        self.sde_net = BSDEApproximateNet(dim, net_hidden_dim, net_depth, net_activation, net_initializer)

    def net_step(self, x: ms.Tensor, t: ms.Tensor):  # (batch, dim) ; (batch, 1)
        r"""
            Calculate the Approximation of u(Xt, t) and the D(u(Xt, t))/D(Xt)

            Inputs:
                - **x** (Tensor) - The Xt with shape=(batch_size, dim).
                - **t** (Tensor) - The t with shape=(batch_size, 1).

            Outputs:
                - **f** (Tensor) - The approximation of u(Xt, t).
                - **grad_f** (Tensor) - The approximation of D(u(Xt, t))/D(Xt).
        """
        f = self.sde_net(x, t)
        grad_f = self.grad_op(self.sde_net)(x, t)
        return f, grad_f

    def _grad_f(self, x):
        r"""
            Calculate the Approximation of the derivative of the backend limitation g_fn(Xt).

            Inputs:
                - **x** (Tensor) - The Xt with shape=(batch_size, dim).

            Outputs:
                - **grad_g** (Tensor) - the approximation of the derivative of the backend limitation g_fn(Xt).
        """
        return self.grad_op(self._g_net)(x)

    def solve(self, et, ew):  # (batch, n_t+1, 1) (batch, n_t+1, dim)
        r"""
            Generate the Approximation Path of the BSDE Problem and Calculate the Value of Loss Function.

            Inputs:
                - **et** (Tensor) - The timestamp t, with shape=(batch_size, n_time+1, 1).
                - **ew** (Tensor) - The accumulated brownian motion value, with shape=(batch_size, n_time+1, 1).

            Outputs:
                - **loss** (Tensor) - The value of the loss function with shape=(1,)
                - **obj** (Tensor) - The approximation sequence of Xt with shape=(batch_size, n_time+1, dim).
                - **der** (Tensor) - The approximation sequence of Yt with shape=(batch_size, n_time+1, 1).
                - **sol** (Tensor) - The solution of the BSDE problem with shape=(1,).
        """
        loss = 0
        t, w = et[:, 0, :], ew[:, 0, :]  # (batch, 1), (batch, dim)
        x = self._r0
        f, grad_f = self.net_step(x, t)
        obj_price, der_price = [x], [f]
        idx = 0
        while idx < self._n_time:
            nt = et[:, idx + 1, :]
            nw = ew[:, idx + 1, :]
            dt = nt - t  # (batch, 1)
            dw = nw - w  # (batch, dim)
            nsigma = self._sigma(t, x, f, self._param)  # (batch_size, dim, dim)
            sigma_w = np.squeeze(np.matmul(nsigma, np.expand_dims(dw, axis=-1)), axis=-1)  # (batch_size, dim)
            nx = x + self._mu(t, x, f, grad_f, self._param) * dt + sigma_w
            nf_euler = f + self._phi(t, x, f, grad_f, self._param) * dt + np.expand_dims(
                (grad_f * sigma_w).sum(axis=1), axis=-1)  # (batch_size, 1)
            nf, ngrad_f = self.net_step(nx, nt)
            loss += np.square(nf_euler - nf).sum()
            t, w, x, f, grad_f = nt, nw, nx, nf, ngrad_f
            obj_price.append(x)
            der_price.append(f)
            idx = idx + 1
        loss += self._weight_g * np.square(f - self._g_net(x)).sum()
        loss += self._weight_g * np.square(grad_f - self._grad_f(x)).sum()
        obj = np.stack(obj_price, axis=1)
        der = np.stack(der_price, axis=1)
        return loss, obj, der, der[0, 0, 0]  # [batch,t+1,dim] [batch,t+1,1] [1]

    def construct(self, et, ew):  # (batch, n_time+1, 1) ; (batch, n_time+1, dim)
        loss, _, _, _ = self.solve(et, ew)
        return loss


# Ref: [1] Maziar Raissi https://arxiv.org/abs/1804.07010
class BSDESolver(ABC):
    def __init__(self, batch_size: int,
                 terminal_time: float,
                 n_time: int,
                 dim: int,
                 r0: ms.Tensor,
                 param: Optional[dict] = None,
                 weight_g: Optional[float] = 1,
                 net_depth: Optional[int] = 4,
                 net_hidden_dim: Optional[int] = 256,
                 net_activation: Optional[nn.Cell] = nn.Tanh(),
                 net_initializer: Optional[str] = 'Normal',
                 seed: Optional[int] = 0,
                 random_type: Optional[int] = 0,
                 dtype=mstype.float32
                 ):
        r"""
            The Solver of the BSDE Problem by Approximating the Unknown Solution u(Xt, t).

            Equation Format of BSDE
            BSDE:
                D(x)=mu_fn*dt+sigma*dw
                D(f)=phi_fn*dt+grad_f*sigma*dw
            Backend Limitation:
                x(t=0)=r0
                f(t=terminal_time)=g_fn(t=terminal_time)

            Args:
                - **batch_size** (int) - The batch_size of the solver.
                - **terminal_time** (float) - The terminal time point of the sde process.
                - **n_time** (int) - The amount of timestamp of the generated path. Please note that t=0 is not included
                                    in this input statistics, which means that the path generated by the solver will
                                    have n_time+1 timestamps with t=0 add to the beginning of path automatically.
                - **dim** (int) - The dimension of the variable Xt.
                - **r0** (Tensor) - The initial state, namely X0.
                - **param** (dict) - Some parameters organized bt the dictionary, which help the calculation of
                                    mu_fn, sigma_fn, phi_fn, g_fn.
                - **weight_g** (float) - Hyper-parameter for the loss function, the weight of backend limitation in the
                                        loss function.
                - **net_depth** (int) - The depth of the u(Xt,t)-approximation MLP. Default: 4.
                - **net_hidden_dim** (int) - The dimension of hidden layer of the u(Xt,t)-approximation MLP.
                                            Default: 256.
                - **net_activation** (nn.Cell) - The activation function of the u(Xt,t)-approximation MLP.
                                                Default: nn.Tanh().
                - **net_initializer** (str) - The initializer of the MLP. Default: 'Normal'.
                - **seed** (int) - The seed of the solver. Default=0.
                - **random_type** (int) - Types of random number sequences.
                                          int[Enum.value] from mind_quant_finance.math.random_ops.
                                          Default: 0(RandomType.PSEUDO).
                - **dtype** (mstype) - The data type of the solver. Default: mstype.float32.

            Reference: [1] Raissi, M. . (2018). Forward-Backward Stochastic Neural Networks: Deep Learning of
                           High-dimensional Partial Differential Equations.  10.48550/arXiv.1804.07010.
        """
        self.dim = dim
        self.n_time = n_time
        self.terminal_time = terminal_time
        self.batch_size = batch_size
        self.param = param
        self.weight_g = weight_g
        interval = terminal_time / float(n_time)
        times = interval * np.arange(n_time + 1)
        times = np.expand_dims(np.expand_dims(times, axis=-1), axis=0)
        times = np.repeat(times, batch_size, axis=0)
        self.times = times.astype(dtype)
        self.interval = ms.Tensor([interval], dtype=dtype)
        # times.shape = (batch, t, 1)
        if r0.ndim == 1 and r0.shape[0] == dim:
            r0 = r0.expand_dims(axis=0)
        elif not (r0.ndim == 2 and r0.shape == (1, dim)):
            raise AttributeError("The Shape of Initial State r0 is Illegal.")
        self.r0 = r0.repeat(batch_size, axis=0).astype(dtype)
        # r0.shape = (batch_size, dim)
        self.element_shape = (batch_size, n_time + 1, dim)
        self.sde_path = BSDEGeneratePath(self.batch_size, self.terminal_time,
                                         self.n_time, self.dim, self.r0,
                                         self.param, self.weight_g,
                                         self.mu_fn, self.sigma_fn, self.phi_fn, self.g_fn,
                                         net_depth, net_hidden_dim, net_activation,
                                         net_initializer, dtype
                                         )
        self.dtype = dtype
        self.seed = seed
        self.random_type = random_type

    def get_path_state(self):
        r"""
            Generate the Timestamp and the Accumulated Brownian Motion Sequence.

            Outputs:
                - **et** (Tensor) - The timestamp t, with shape=(batch_size, n_time+1, 1).
                - **ew** (Tensor) - The accumulated brownian motion value, with shape=(batch_size, n_time+1, 1).
        """
        w = generate_mc_normal_draws(self.element_shape, self.random_type, self.seed, self.dtype)
        w = np.sqrt(self.interval) * ms.Tensor(w, dtype=self.dtype)
        return self.times, np.cumsum(w, axis=1)  # et(batch, t+1, 1) ew(batch, t+1, dim)

    def train(self, epochs, learning_rate, per_print_times=100, verbose=True):
        r"""
            Train the BSDE Solver.

            Inputs:
                - **epochs** (int) - The epoch of training process.
                - **learning_rate** (float or list) - The learning rate of the Adam optimizer. If the type is list, then
                                                      the length of list should be bigger than epochs.
                - **per_print_times** (int) - The intersection epochs between per training information print.
                - **verbose** (bool) - The verbose control. If set to False, then no training information will be print.
        """
        optimizer = nn.Adam(self.sde_path.trainable_params(), learning_rate=learning_rate)
        train_one_step = nn.TrainOneStepCell(network=self.sde_path, optimizer=optimizer)
        train_one_step.set_train()
        for epoch in range(epochs):
            et, ew = self.get_path_state()
            loss = train_one_step(et, ew)
            if verbose and epoch % per_print_times == 0:
                if type(learning_rate) == list:
                    lr = learning_rate[epoch]
                else:
                    lr = learning_rate
                print(f"Epoch: [{epoch} / {epochs}] loss: {loss} learning_rate={lr}")

    def approximate_path(self, et, ew):  # (batch_size, t+1, [1]), (batch_size, t+1, dim)
        r"""
            Generate the Approximation of the {Xt}, {Yt} and the Solution of the BSDE Problem.

            Inputs:
                - **et** (Tensor) - The timestamp t, with shape=(batch_size, n_time+1, 1).
                - **ew** (Tensor) - The accumulated brownian motion value, with shape=(batch_size, n_time+1, 1).

            Outputs:
                - **obj** (Tensor) - The approximation sequence of Xt with shape=(batch_size, n_time+1, dim).
                - **der** (Tensor) - The approximation sequence of Yt with shape=(batch_size, n_time+1, 1).
                - **sol** (Tensor) - The solution of the BSDE problem with shape=(1,).
        """
        if et.ndim == 2:
            et = np.expand_dims(et, axis=-1)
        # Param: Accumulate TimeStamp & BrownVariable
        # Restriction: et[n_time] == self.terminal_time
        _, obj, der, der_price = self.sde_path.solve(et, ew)
        return obj, der, der_price  # (batch_size, t+1, dim), (batch_size, t+1, 1), float

    def approximate_solution(self, et, ex):  # ([batch_size], 1), ([batch_size], dim)
        r"""
            Generate the Approximation of the Solution of the BSDE Problem At any Timestamp.

            Inputs:
                - **et** (Tensor) - The timestamp t, with shape=(batch_size, 1).
                - **ew** (Tensor) - The accumulated brownian motion value, with shape=(batch_size, dim).

            Outputs:
                - **sol** (Tensor) - The solution of the BSDE problem with shape=(1,).
        """
        # Param: TimeStamp(<terminal_time) & current_object_price
        # Restriction: et <= self.terminal_time
        return self.sde_path.sde_net(ex, et)

    @abstractmethod
    def mu_fn(self, t, x, f, grad_f, param):  # (batch,1),(batch,dim),(batch,1),(batch,dim)
        r"""
            The mu(t, x, y, z) Function of the BSDE Problem.
            This is an Abstract Method needed to be Realized by the Users.

            Inputs:
                - **t** (Tensor) - The t of the input of mu() with shape=(batch_size, 1)
                - **x** (Tensor) - The x of the input of mu() with shape=(batch_size, dim)
                - **f** (Tensor) - The y of the input of mu() with shape=(batch_size, 1)
                - **grad_f** (Tensor) - The z of the input of mu() with shape=(batch_size, dim)
                - **param** (dict) - Parameters organized bt the dictionary, which help the calculation of mu_fn.

            Outputs:
                - **val** (Tensor) - the value of mu(t, x, f, grad_f) with shape = (batch, dim).

            Reference: [1] Raissi, M. . (2018). Forward-Backward Stochastic Neural Networks: Deep Learning of
                           High-dimensional Partial Differential Equations.  10.48550/arXiv.1804.07010.
        """
        return np.zeros((self.batch_size, self.dim), dtype=self.dtype)

    @abstractmethod
    def sigma_fn(self, t, x, f, param):  # (batch,1),(batch,dim),(batch,1)
        r"""
            The sigma(t, x, y) Function of the BSDE Problem.
            This is an Abstract Method needed to be Realized by the Users.

            Inputs:
                - **t** (Tensor) - The t of the input of sigma() with shape=(batch_size, 1)
                - **x** (Tensor) - The x of the input of sigma() with shape=(batch_size, dim)
                - **f** (Tensor) - The y of the input of sigma() with shape=(batch_size, 1)
                - **param** (dict) - Parameters organized bt the dictionary, which help the calculation of sigma_fn.

            Outputs:
                - **val** (Tensor) - the value of sigma(t, x, f) with with shape = (batch, dim, dim).

            Reference: [1] Raissi, M. . (2018). Forward-Backward Stochastic Neural Networks: Deep Learning of
                           High-dimensional Partial Differential Equations.  10.48550/arXiv.1804.07010.
        """
        return np.zeros((self.batch_size, self.dim, self.dim), dtype=self.dtype)

    @abstractmethod
    def phi_fn(self, t, x, f, grad_f, param):  # (batch,1),(batch,dim),(batch,1),(batch,dim)
        r"""
            The phi(t, x, y, z) Function of the BSDE Problem.
            This is an Abstract Method needed to be Realized by the Users.

            Inputs:
                - **t** (Tensor) - The t of the input of phi() with shape=(batch_size, 1)
                - **x** (Tensor) - The x of the input of phi() with shape=(batch_size, dim)
                - **f** (Tensor) - The y of the input of phi() with shape=(batch_size, 1)
                - **grad_f** (Tensor) - The z of the input of phi() with shape=(batch_size, dim)
                - **param** (dict) - Parameters organized bt the dictionary, which help the calculation of phi_fn.

            Outputs:
                - **val** (Tensor) - the value of phi(t, x, f, grad_f) with shape = (batch, 1).

            Reference: [1] Raissi, M. . (2018). Forward-Backward Stochastic Neural Networks: Deep Learning of
                           High-dimensional Partial Differential Equations.  10.48550/arXiv.1804.07010.
        """
        return np.zeros((self.batch_size, 1), dtype=self.dtype)

    @abstractmethod
    def g_fn(self, x, param):  # (batch,dim)
        r"""
            The Backend Limitation g(t, x) Function of the BSDE Problem.
            This is an Abstract Method needed to be Realized by the Users.

            Inputs:
                - **x** (Tensor) - The x of the input of g() with shape=(batch_size, dim)
                - **param** (dict) - Parameters organized bt the dictionary, which help the calculation of g_fn.

            Outputs:
                - **val** (Tensor) - the value of g(x) with shape = (batch, 1)

            Reference: [1] Raissi, M. . (2018). Forward-Backward Stochastic Neural Networks: Deep Learning of
                           High-dimensional Partial Differential Equations.  10.48550/arXiv.1804.07010.
        """
        return np.zeros((self.batch_size, 1), dtype=self.dtype)